
 =================== # Dataset info # =================== 
Data source: kits - sketch
Feature: bin
Data Portion: 37
Sketching method: pminhash
Sampling k: 1024
Using Counsketch: c = 4
Train A shape: (700, 1228), Train B shape: (700, 2868), label shape: (700,)
Test data shape: (300, 4096), label shape: (300,)
 =================== # Training info # =================== 
batch size: 20
alpha: 0.001
max_iter: 100
WAN_bandwidth: 10 Mbps
mem_occupancy: 4 Byte
 =================== #   Info End   # =================== 

Epoch 1, batch sum loss: 0.695350444933146 Time: 2.1123898029327393s
Epoch 2, batch sum loss: 0.6952493469212362 Time: 2.106462001800537s
Epoch 3, batch sum loss: 0.695162479543816 Time: 2.1125783920288086s
Epoch 4, batch sum loss: 0.6950782696004042 Time: 2.20991849899292s
Epoch 5, batch sum loss: 0.6949964937944685 Time: 2.0710830688476562s
Epoch 6, batch sum loss: 0.6949170105183706 Time: 2.2585036754608154s
Epoch 7, batch sum loss: 0.694839674939145 Time: 1.9999945163726807s
Epoch 8, batch sum loss: 0.6947643429069281 Time: 2.0252928733825684s
Epoch 9, batch sum loss: 0.6946909097199052 Time: 2.148834466934204s
Epoch 10, batch sum loss: 0.6946192512190036 Time: 2.1271352767944336s
Epoch 11, batch sum loss: 0.6945493025259096 Time: 1.9837002754211426s
Epoch 12, batch sum loss: 0.6944809582627488 Time: 1.9165563583374023s
Epoch 13, batch sum loss: 0.6944141339182289 Time: 2.0120885372161865s
Epoch 14, batch sum loss: 0.694348779942283 Time: 1.9770259857177734s
Epoch 15, batch sum loss: 0.6942848151006887 Time: 1.9826891422271729s
Epoch 16, batch sum loss: 0.694222178387333 Time: 1.9826140403747559s
Epoch 17, batch sum loss: 0.694160827173573 Time: 2.206218719482422s
Epoch 18, batch sum loss: 0.6941006992387512 Time: 1.9585158824920654s
Epoch 19, batch sum loss: 0.6940417470886832 Time: 2.0394482612609863s
Epoch 20, batch sum loss: 0.6939839370801206 Time: 1.99757719039917s
Epoch 21, batch sum loss: 0.6939272116969954 Time: 1.9516210556030273s
Epoch 22, batch sum loss: 0.6938715493558705 Time: 2.0504729747772217s
Epoch 23, batch sum loss: 0.6938169039507409 Time: 1.930762529373169s
Epoch 24, batch sum loss: 0.6937632392819261 Time: 2.038320779800415s
Epoch 25, batch sum loss: 0.6937105434096611 Time: 2.0030014514923096s
Epoch 26, batch sum loss: 0.6936587589290796 Time: 1.9786334037780762s
Epoch 27, batch sum loss: 0.6936078755697289 Time: 2.035547971725464s
Epoch 28, batch sum loss: 0.6935578573279997 Time: 2.1550073623657227s
Epoch 29, batch sum loss: 0.6935086798270464 Time: 1.9808008670806885s
Epoch 30, batch sum loss: 0.6934603303674235 Time: 2.0256078243255615s
Epoch 31, batch sum loss: 0.6934127721932422 Time: 2.053224802017212s
Epoch 32, batch sum loss: 0.6933659922141036 Time: 1.9562840461730957s
Epoch 33, batch sum loss: 0.6933199617971277 Time: 2.0088839530944824s
Epoch 34, batch sum loss: 0.6932746700127195 Time: 2.019888401031494s
Epoch 35, batch sum loss: 0.6932300876458367 Time: 2.0169529914855957s
Epoch 36, batch sum loss: 0.6931862002746596 Time: 2.065274477005005s
Epoch 37, batch sum loss: 0.6931429947860496 Time: 2.296443462371826s
Epoch 38, batch sum loss: 0.6931004527520876 Time: 2.2954864501953125s
Epoch 39, batch sum loss: 0.6930585576152812 Time: 2.033419609069824s
Epoch 40, batch sum loss: 0.6930172923808865 Time: 2.255685329437256s
Epoch 41, batch sum loss: 0.6929766397597542 Time: 2.880465030670166s
Epoch 42, batch sum loss: 0.6929365902824519 Time: 2.279963493347168s
Epoch 43, batch sum loss: 0.6928971285952552 Time: 1.9772253036499023s
Epoch 44, batch sum loss: 0.6928582412843634 Time: 1.9694347381591797s
Epoch 45, batch sum loss: 0.6928199188946936 Time: 2.9923582077026367s
Epoch 46, batch sum loss: 0.6927821391726652 Time: 2.8839070796966553s
Epoch 47, batch sum loss: 0.6927449000561541 Time: 2.255133628845215s
Epoch 48, batch sum loss: 0.6927081850839238 Time: 2.065904378890991s
Epoch 49, batch sum loss: 0.6926719855850046 Time: 2.028167247772217s
Epoch 50, batch sum loss: 0.6926362880102517 Time: 2.132127285003662s
Epoch 51, batch sum loss: 0.6926010791732309 Time: 2.3735034465789795s
Epoch 52, batch sum loss: 0.6925663592554032 Time: 2.34486722946167s
Epoch 53, batch sum loss: 0.6925321084834393 Time: 2.1266298294067383s
Epoch 54, batch sum loss: 0.6924983176864092 Time: 2.0638108253479004s
Epoch 55, batch sum loss: 0.6924649893373023 Time: 2.3292031288146973s
Epoch 56, batch sum loss: 0.6924320949668327 Time: 2.006890296936035s
Epoch 57, batch sum loss: 0.6923996391516107 Time: 2.6564910411834717s
Epoch 58, batch sum loss: 0.6923676053907947 Time: 2.6084561347961426s
Epoch 59, batch sum loss: 0.6923359973201968 Time: 2.1041994094848633s
Epoch 60, batch sum loss: 0.6923047952957554 Time: 2.046224355697632s
Epoch 61, batch sum loss: 0.6922740008141843 Time: 2.0893099308013916s
Epoch 62, batch sum loss: 0.6922435936873538 Time: 2.102656364440918s
Epoch 63, batch sum loss: 0.6922135751434677 Time: 2.062448501586914s
Epoch 64, batch sum loss: 0.6921839369201671 Time: 1.9660415649414062s
Epoch 65, batch sum loss: 0.6921546723361539 Time: 2.265894889831543s
Epoch 66, batch sum loss: 0.692125769237126 Time: 2.5792648792266846s
Epoch 67, batch sum loss: 0.6920972282669421 Time: 2.1648712158203125s
Epoch 68, batch sum loss: 0.692069037222117 Time: 2.2553746700286865s
Epoch 69, batch sum loss: 0.6920411927575852 Time: 2.8065836429595947s
Epoch 70, batch sum loss: 0.6920136922993289 Time: 2.252140522003174s
Epoch 71, batch sum loss: 0.6919865182191007 Time: 2.037317991256714s
Epoch 72, batch sum loss: 0.6919596763277336 Time: 2.0867502689361572s
Epoch 73, batch sum loss: 0.6919331500974001 Time: 2.065768241882324s
Epoch 74, batch sum loss: 0.6919069427600846 Time: 2.5401923656463623s
Epoch 75, batch sum loss: 0.6918810487021616 Time: 2.632697343826294s
Epoch 76, batch sum loss: 0.6918554502847988 Time: 2.154334783554077s
Epoch 77, batch sum loss: 0.6918301625346501 Time: 2.019015073776245s
Epoch 78, batch sum loss: 0.6918051576810827 Time: 2.009960412979126s
Epoch 79, batch sum loss: 0.6917804436523953 Time: 2.032071828842163s
Epoch 80, batch sum loss: 0.6917560205374432 Time: 2.0546200275421143s
Epoch 81, batch sum loss: 0.6917318720704783 Time: 2.0312650203704834s
Epoch 82, batch sum loss: 0.6917079975672444 Time: 1.992401123046875s
Epoch 83, batch sum loss: 0.6916843938874088 Time: 2.0895233154296875s
Epoch 84, batch sum loss: 0.6916610519921935 Time: 1.945495843887329s
Epoch 85, batch sum loss: 0.6916379711442776 Time: 2.253649950027466s
Epoch 86, batch sum loss: 0.6916151484043447 Time: 2.2165894508361816s
Epoch 87, batch sum loss: 0.691592581302431 Time: 1.8952813148498535s
Epoch 88, batch sum loss: 0.6915702520555714 Time: 2.074380397796631s
Epoch 89, batch sum loss: 0.6915481719849044 Time: 2.2341437339782715s
Epoch 90, batch sum loss: 0.6915263355482065 Time: 2.136995792388916s
Epoch 91, batch sum loss: 0.691504732169032 Time: 2.02219820022583s
Epoch 92, batch sum loss: 0.6914833552613897 Time: 2.0370960235595703s
Epoch 93, batch sum loss: 0.6914622069306067 Time: 1.7186720371246338s
Epoch 94, batch sum loss: 0.6914412866083595 Time: 1.723184585571289s
Epoch 95, batch sum loss: 0.6914205897762056 Time: 1.6251952648162842s
Epoch 96, batch sum loss: 0.6914001058073475 Time: 2.2192788124084473s
Epoch 97, batch sum loss: 0.6913798409503524 Time: 2.0987021923065186s
Epoch 98, batch sum loss: 0.6913597707329868 Time: 2.114274024963379s
Epoch 99, batch sum loss: 0.6913399270050004 Time: 2.010256290435791s
Epoch 100, batch sum loss: 0.6913202779070459 Time: 2.0438146591186523s

# ================== #  Test Model  # ================== #
score: 218
len(y): 300
Predict precision: 0.7266666666666667
# ================== #   Train Time   # ================== #
SecureMLModel comm_time account: 167.19378662107917
Total time cost: 407.23219275473093 s